# -*- coding: utf-8 -*-
"""MBTIAIModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bxZs4ZIlz4HDiIp_qLsVbNtvlnk9urXh
"""

# GUI REQUIREMENTS (Streamlit.io)

# General imports

!pip install catboost
import pandas as pd
from catboost import CatBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from wordcloud import WordCloud
from tqdm import tqdm
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import re
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC,LinearSVC
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingClassifier
from imblearn.over_sampling import SMOTE
import plotly.express as px
import warnings
warnings.filterwarnings('ignore')

# Reading
data=pd.read_csv('/home/mbti_1.csv')
data.head()

data.describe(include='O')

data['type'].value_counts()

# Must remove links in order to specify just text
data.posts[0]

# Splits training data and testing data into 2 different splits
# Training data: 80%
# Testing data: 20%
train_data,test_data=train_test_split(data,test_size=0.2,random_state=35,stratify=data.type)

# Use simple function to clean up 'https?://[^\s<>"]+|www\.[^\s<>"]+'

def clear_text(data):
    data_length=[]
    lemmatizer=WordNetLemmatizer()
    cleaned_text=[]
    for sentence in tqdm(data.posts):
        sentence=sentence.lower()

#        removing links from text data
        sentence=re.sub('https?://[^\s<>"]+|www\.[^\s<>"]+',' ',sentence)

#        removing other symbols
        sentence=re.sub('[^0-9a-z]',' ',sentence)


        data_length.append(len(sentence.split()))
        cleaned_text.append(sentence)
    return cleaned_text,data_length

# Using the function on train_data.posts

train_data.posts,train_length=clear_text(train_data)
test_data.posts,test_length=clear_text(test_data)

# VISUALIZING DATA

plt.figure(figsize=(15,10))
sns.distplot(train_length,label='train data word length')
sns.distplot(test_length,label='test data word length')
plt.title('Number of words in text',fontdict={'size':20,'style':'italic'})
plt.show()

px.pie(train_data,names='type',title='Personality type',hole=0.3)

# TOKENIZING THE POSTS
# - Breaks words down into smaller units called tokens
# - Will be using Lemmatization:
# Sentence: "The quick brown fox jumps over the lazy dog."
# Lemmatization: "the", "quick", "brown", "fox", "jump", "over", "the", "lazy", "dog"

!pip install nltk
import nltk
nltk.download('wordnet')
# Setting up Lemmatizer
class Lemmatizer(object):
    def __init__(self):
        self.lemmatizer = WordNetLemmatizer()
    def __call__(self, sentence):
        return [self.lemmatizer.lemmatize(word) for word in sentence.split() if len(word)>2]

# Vectorize Data
# Fitting data to a vectorizer object
vectorizer=TfidfVectorizer( max_features=5000,stop_words='english',tokenizer=Lemmatizer())
vectorizer.fit(train_data.posts)

# Vectorizing the training posts and the testing posts
train_post=vectorizer.transform(train_data.posts).toarray()
test_post=vectorizer.transform(test_data.posts).toarray()
train_post.shape

target_encoder=LabelEncoder()
train_target=target_encoder.fit_transform(train_data.type)
test_target=target_encoder.fit_transform(test_data.type)

# TESTING AND SELECTING MODELS
# - Creating array to see model accuracy
# - Testing different models on same dataset and looking for lowest error
models_accuracy={}

# Model #1: Logistic Regression
# - Uses logistic regression function to predict a linear relationship between predictions and guesses

# Fitting training data to logistic regression model
model_log=LogisticRegression(max_iter=3000,C=0.5,n_jobs=-1)
model_log.fit(train_post,train_target)

# Training accuracy report
print('train classification report \n ',classification_report(train_target,model_log.predict(train_post),target_names=target_encoder.inverse_transform([i for i in range(16)])))

# Testing accuracy report
print('test classification report \n',classification_report(test_target,model_log.predict(test_post),target_names=target_encoder.inverse_transform([i for i in range(16)])))

#Logging accuracy report into array
models_accuracy['logistic regression']=accuracy_score(test_target,model_log.predict(test_post))

# Model #2: Linear Support Vector classifier
# - Finds maximum-margin hyperplane that seperates data points while minimizing classification errors

#Training model
model_linear_svc=LinearSVC(C=0.1)
model_linear_svc.fit(train_post,train_target)

print('Train classification report \n ',classification_report(train_target,model_linear_svc.predict(train_post),target_names=target_encoder.inverse_transform([i for i in range(16)])))

print('Test classification report \n',classification_report(test_target,model_linear_svc.predict(test_post),target_names=target_encoder.inverse_transform([i for i in range(16)])))

# Logging model accuracy into array
models_accuracy['Linear Support Vector classifier']=accuracy_score(test_target,model_linear_svc.predict(test_post))

# Model #3: Support Vector classifier
# - Trained using a hinge loss function, which penalizes misclassified data points

model_svc=SVC()
model_svc.fit(train_post,train_target)

print('train classification report \n ',classification_report(train_target,model_svc.predict(train_post),target_names=target_encoder.inverse_transform([i for i in range(16)])))

print('test classification report \n ',classification_report(test_target,model_svc.predict(test_post),target_names=target_encoder.inverse_transform([i for i in range(16)])))

models_accuracy['Support Vector classifier']=accuracy_score(test_target,model_svc.predict(test_post))

# Model #4: Multinomial Naive Bayes
# - Probabilistic classifier trained to use a maximum likihood estimation approach

model_multinomial_nb=MultinomialNB()
model_multinomial_nb.fit(train_post,train_target)

print('train classification report \n ',classification_report(train_target,model_multinomial_nb.predict(train_post),target_names=target_encoder.inverse_transform([i for i in range(16)])))

print('test classification report \n ',classification_report(test_target,model_multinomial_nb.predict(test_post),target_names=target_encoder.inverse_transform([i for i in range(16)])))

models_accuracy['Multinomial Naive Bayes']=accuracy_score(test_target,model_multinomial_nb.predict(test_post))

# Model #5: Decision Tree classifier
# - recursively splits data into smaller and smaller subsets based on values of each feature

model_tree=DecisionTreeClassifier(max_depth=14)
model_tree.fit(train_post,train_target)

print('train classification report \n ',classification_report(train_target,model_tree.predict(train_post),target_names=target_encoder.inverse_transform([i for i in range(16)])))

print('test classification report \n ',classification_report(test_target,model_tree.predict(test_post),target_names=target_encoder.inverse_transform([i for i in range(16)])))

models_accuracy['Decision Tree classifier']=accuracy_score(test_target,model_tree.predict(test_post))

# Model #6: Random Forest Classifier
# - Uses multiple decision trees (a forest) and averages their predictions

model_forest=RandomForestClassifier(max_depth=10)
model_forest.fit(train_post,train_target)

print('train classification report \n ',classification_report(train_target,model_forest.predict(train_post),target_names=target_encoder.inverse_transform([i for i in range(16)])))

print('test classification report \n ',classification_report(test_target,model_forest.predict(test_post),target_names=target_encoder.inverse_transform([i for i in range(16)])))

models_accuracy['Random Forest Classifier']=accuracy_score(test_target,model_forest.predict(test_post))

# With all model accuracy being logged, now we can decide which model to use
models_accuracy

# Create pandas dataframe from models_accuracy
accuarcy=pd.DataFrame(models_accuracy.items(),columns=['Models','Test accuracy'])

accuarcy.sort_values(by='Test accuracy',ascending=False,ignore_index=True).style.background_gradient(cmap='Blues')

# Linear Support Vector Classifier seems to do the best at predicting

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder
from nltk.stem import WordNetLemmatizer
import re
import string

class Lemmatizer:
    def __init__(self):
        self.lemmatizer = WordNetLemmatizer()

    def __call__(self, doc):
        return [self.lemmatizer.lemmatize(word) for word in doc.split()]

# Replace ... with your actual TfidfVectorizer and Logistic Regression model
vectorizer = TfidfVectorizer(max_features=10000, stop_words='english', tokenizer=Lemmatizer())
model_log = LogisticRegression(max_iter=3000, C=0.5, n_jobs=-1)

# Assuming train_data is your training dataset
vectorizer.fit(train_data.posts)
train_post = vectorizer.transform(train_data.posts)

target_encoder = LabelEncoder()
train_target = target_encoder.fit_transform(train_data.type)

model_log.fit(train_post, train_target)

def preprocess_text(text):
    # Function to preprocess input text (similar to what was done during training)
    text = text.lower()
    text = re.sub(f"[{re.escape(string.punctuation)}]", "", text)
    return text

def predict_mbti(text):
    # Function to predict MBTI from input text
    preprocessed_text = preprocess_text(text)
    vectorized_text = vectorizer.transform([preprocessed_text])
    probabilities = model_log.predict_proba(vectorized_text)[0]
    prediction = model_log.predict(vectorized_text)[0]
    confidence_percentage = probabilities[prediction] * 100
    return target_encoder.inverse_transform([prediction])[0], confidence_percentage

# Get input from the user
input_text = input("Enter text for MBTI prediction: ")

# Predict and print the result with confidence
prediction, confidence = predict_mbti(input_text)
print(f"Predicted MBTI: {prediction}")
print(f"Confidence: {confidence:.2f}%")

!pip install streamlit
!streamlit hello